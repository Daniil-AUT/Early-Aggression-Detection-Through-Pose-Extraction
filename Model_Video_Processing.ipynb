{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f4094a-ef70-46a2-bc41-90198dd0c015",
   "metadata": {},
   "source": [
    "# Real-Time Violence Detection System\n",
    "\n",
    "\n",
    "### Overview\n",
    "This notebook tries to apply real-time violence detection system using pose estimation and deep learning. The system processes video streams to detect violent activities with visual outputs.\n",
    "\n",
    "### Key Features\n",
    "- **YOLOv8 Pose Estimation**: Multi-person pose detection\n",
    "- **Biomechanical Feature Extraction**: 10-dimensional feature space\n",
    "- **Real-time Processing**: Live webcam or video file input\n",
    "- **Time series graph**: Time-series graphs and keyframe capture\n",
    "\n",
    "### System Architecture\n",
    "- Fixed 75-frame processing window\n",
    "- Savitsky-Golay feature smoothing\n",
    "- Multi-person tracking\n",
    "- Violence probability scoring\n",
    "- Multiple visualisation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33448e31-2887-48ff-bd81-d86e7439785e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51cc02f-e565-4237-af70-145998bcd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "TensorFlow version: 2.10.1\n",
      "OpenCV version: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from ultralytics import YOLO\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import euclidean\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edba8b-076f-4d43-a9a1-38d2c33d7b4c",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecc1903-9bac-46a4-8432-8117b3ad10f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"models/violence_detection_slr_aligned.keras\"\n",
    "SCALER_PATH = \"extracted_features_slr_aligned/slr_aligned_scaler.pkl\"\n",
    "\n",
    "# Violence levels configuration\n",
    "VIOLENCE_LEVELS = {\n",
    "    0: (\"Normal\", (0, 0.20), (0, 255, 0)),\n",
    "    1: (\"Early Aggression\", (0.21, 0.40), (0, 255, 255)),\n",
    "    2: (\"Pre-Violence\", (0.41, 0.60), (0, 165, 255)),\n",
    "    3: (\"High Possibility\", (0.61, 0.80), (0, 0, 255)),\n",
    "    4: (\"Active Violence\", (0.81, 1.00), (0, 0, 139))\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ceb12a-ea97-4052-921a-60d9145d38f6",
   "metadata": {},
   "source": [
    "# Core Detector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421a829f-a637-4508-9ffb-c66f69abbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector class defined\n"
     ]
    }
   ],
   "source": [
    "class ViolenceDetector:\n",
    "    def __init__(self, model_path, scaler_path):\n",
    "        # Load model and scaler\n",
    "        self.model = load_model(model_path)\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        # Initialise YOLO\n",
    "        self.pose_model = YOLO(\"yolov8l-pose.pt\")\n",
    "        \n",
    "        # Training-aligned parameters\n",
    "        self.CONFIDENCE_THRESHOLD = 0.25\n",
    "        self.FEATURE_DIM = 10\n",
    "        self.sequence_length = 75  # Fixed to match training\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.person_sequences = defaultdict(lambda: deque(maxlen=150))\n",
    "        self.tracked_persons = {}\n",
    "        self.next_person_id = 0\n",
    "        \n",
    "        # Violence tracking\n",
    "        self.max_violence_prob = 0.0\n",
    "        self.max_violence_level = \"Normal\"\n",
    "        \n",
    "        # Visualisation\n",
    "        self.violence_history = []\n",
    "        self.keyframe_captures = []\n",
    "        \n",
    "        print(\"Detector initialised successfully\")\n",
    "\n",
    "print(\"Detector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edea95-4067-4844-a7a6-cb8bff646e16",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79c66ad-658a-4fed-88ca-42200ffb1a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector class defined\n"
     ]
    }
   ],
   "source": [
    "class ViolenceDetector:\n",
    "    def __init__(self, model_path, scaler_path):\n",
    "        # Load model and scaler\n",
    "        self.model = load_model(model_path)\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        # Initialise YOLO\n",
    "        self.pose_model = YOLO(\"yolov8l-pose.pt\")\n",
    "        \n",
    "        # Training-aligned parameters\n",
    "        self.CONFIDENCE_THRESHOLD = 0.25\n",
    "        self.FEATURE_DIM = 10\n",
    "        self.sequence_length = 75  # Fixed to match training\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.person_sequences = defaultdict(lambda: deque(maxlen=150))\n",
    "        self.tracked_persons = {}\n",
    "        self.next_person_id = 0\n",
    "        \n",
    "        # Violence tracking\n",
    "        self.max_violence_prob = 0.0\n",
    "        self.max_violence_level = \"Normal\"\n",
    "        \n",
    "        # Visualisation\n",
    "        self.violence_history = []\n",
    "        self.keyframe_captures = []\n",
    "        \n",
    "        print(\"Detector initialised successfully\")\n",
    "\n",
    "print(\"Detector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b53dff-add9-41b2-bff7-bb331679f91b",
   "metadata": {},
   "source": [
    "# Processing and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b974bb3b-ffcf-4bf0-879b-5f2bbae250cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing methods added\n"
     ]
    }
   ],
   "source": [
    "class ViolenceDetector(ViolenceDetector):\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process single frame for violence detection\"\"\"\n",
    "        # YOLO pose detection\n",
    "        results = self.pose_model(frame, conf=self.CONFIDENCE_THRESHOLD, verbose=False)\n",
    "        \n",
    "        if not results or not hasattr(results[0], 'keypoints'):\n",
    "            return\n",
    "        \n",
    "        # Process detections and extract features\n",
    "        # ... (feature extraction and tracking logic)\n",
    "        \n",
    "        # Make predictions when buffer is full\n",
    "        if self.sequence_length and len(self.person_sequences[person_id]) >= self.sequence_length:\n",
    "            probability = self.predict_violence(person_id)\n",
    "            self.update_violence_levels(person_id, probability)\n",
    "    \n",
    "    def predict_violence(self, person_id):\n",
    "        \"\"\"Predict violence probability using trained model\"\"\"\n",
    "        sequence = list(self.person_sequences[person_id])[-self.sequence_length:]\n",
    "        sequence = np.array(sequence, dtype=np.float32)\n",
    "        \n",
    "        # Apply scaler and reshape\n",
    "        sequence_flat = sequence.reshape(-1, self.FEATURE_DIM)\n",
    "        sequence_scaled = self.scaler.transform(sequence_flat)\n",
    "        sequence_reshaped = sequence_scaled.reshape(1, self.sequence_length, self.FEATURE_DIM)\n",
    "        \n",
    "        # Model prediction\n",
    "        prediction = self.model.predict(sequence_reshaped, verbose=0)\n",
    "        return float(prediction[0][0])\n",
    "\n",
    "print(\"Processing methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4075f-564c-422c-b5d2-9c56bfbc95d6",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86cff65-fd9a-4e40-999c-a3ca78f3bd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualisation methods added\n"
     ]
    }
   ],
   "source": [
    "class ViolenceDetector(ViolenceDetector):\n",
    "    def draw_visualisation(self, frame):\n",
    "        \"\"\"Draw detection results on frame\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Draw bounding boxes and skeletons\n",
    "        for person_id, person_data in self.tracked_persons.items():\n",
    "            bbox = person_data['bbox']\n",
    "            violence_info = self.person_violence_levels.get(person_id, {\"prob\": 0.0, \"level\": \"Normal\", \"color\": (0, 255, 0)})\n",
    "            \n",
    "            # Draw bounding box\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), violence_info[\"color\"], 3)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"ID:{person_id} {violence_info['level']} ({violence_info['prob']:.3f})\"\n",
    "            cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "        \n",
    "        # Add global info panel\n",
    "        cv2.rectangle(frame, (10, 10), (400, 100), (0,0,0), -1)\n",
    "        cv2.putText(frame, f\"MAX: {self.max_violence_level} ({self.max_violence_prob:.3f})\", \n",
    "                   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "print(\"Visualisation methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1b6af-d564-4d58-9396-24b8692cafd0",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2e3132-7871-4475-b33b-41f4c2a980a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main function defined\n"
     ]
    }
   ],
   "source": [
    "def run_detection():\n",
    "    \"\"\"Main function to run violence detection\"\"\"\n",
    "    # Check files exist\n",
    "    if not all(os.path.exists(p) for p in [MODEL_PATH, SCALER_PATH]):\n",
    "        print(\"ERROR: Model or scaler not found. Please train model first.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = ViolenceDetector(MODEL_PATH, SCALER_PATH)\n",
    "    \n",
    "    # Input selection\n",
    "    print(\"Select input source:\")\n",
    "    print(\"1. Webcam\")\n",
    "    print(\"2. Video file\")\n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    elif choice == \"2\":\n",
    "        video_path = input(\"Enter video path: \").strip()\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n",
    "        return\n",
    "    \n",
    "    print(\"Starting detection... Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        detector.process_frame(frame)\n",
    "        \n",
    "        # Draw results\n",
    "        frame = detector.draw_visualization(frame)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Violence Detection', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Detection completed\")\n",
    "\n",
    "print(\"Main function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c5c0c-8781-4f9e-9082-70b394f0f7d0",
   "metadata": {},
   "source": [
    "# Run the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287a011-ea9e-45ae-9080-a980fd2907d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9c434-3a3f-4816-bf6a-84bb0c58a384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF GPU Env",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
