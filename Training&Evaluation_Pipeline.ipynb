{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c2a2c5-06f1-4210-8999-09bf6ea74ff7",
   "metadata": {},
   "source": [
    "# SLR-Aligned Violence Detection System\n",
    "## Academic Implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a  violence detection system aligned with Systematic Literature Review (SLR) methodology.\n",
    "\n",
    "---\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Core Components\n",
    "- **YOLOv8 Pose Estimation** with multi-person tracking\n",
    "- **Biomechanical Feature Extraction** (angles, deltas, spatial metrics)\n",
    "- **BiLSTM with Attention Mechanism** for temporal modeling\n",
    "- **BoTSORT-inspired Tracking** for multi-person scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Target\n",
    "\n",
    "| Metric    | Target  |\n",
    "|-----------|---------|\n",
    "| Accuracy  | 80-90%  |\n",
    "| Dataset   | RWF-2000|\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- Violence detection in multi-person scenarios\n",
    "- Interpretable biomechanical feature analysis\n",
    "- Temporal pattern recognition via attention mechanisms\n",
    "- Reproducibility\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e97ce-c98b-40cc-9a5c-cfb99f099a55",
   "metadata": {},
   "source": [
    "# Section 1: Environment Setup & Imports\n",
    "## System Configuration and Dependency Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98402cf-4ecc-453b-a235-fc05506e104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports completed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, \n",
    "                             precision_recall_curve, roc_auc_score, \n",
    "                             average_precision_score, f1_score, matthews_corrcoef,\n",
    "                             precision_score, recall_score, accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# YOLO and OpenCV\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import deque, defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ['YOLO_VERBOSE'] = 'False'\n",
    "logging.getLogger('ultralytics').setLevel(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b647c-e375-435c-9903-5dad0b3a66e0",
   "metadata": {},
   "source": [
    "# Section 2: GPU Configuration\n",
    "## Hardware Acceleration Setup\n",
    "\n",
    "Configures GPU acceleration for TensorFlow and PyTorch environments. \n",
    "\n",
    "**Key Actions:**\n",
    "- TensorFlow GPU memory configuration\n",
    "- PyTorch CUDA device detection  \n",
    "- YOLO device assignment (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e868cb-04bc-4724-8244-6ea3240c1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GPU CONFIGURATION\n",
      "======================================================================\n",
      "Physical GPUs: 1, Logical GPUs: 1\n",
      "\n",
      "PyTorch version: 1.12.1+cu113\n",
      "CUDA available: True\n",
      "PyTorch CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TensorFlow GPU setup\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected - using CPU\")\n",
    "\n",
    "# PyTorch GPU setup\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    YOLO_DEVICE = 0\n",
    "else:\n",
    "    print(\"PyTorch CUDA not available - YOLO will use CPU\")\n",
    "    YOLO_DEVICE = 'cpu'\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832ea5e-fae9-48f1-b04a-82bccf50c121",
   "metadata": {},
   "source": [
    "# Section 3: Configuration Parameters\n",
    "## System Constants and Paths\n",
    "\n",
    "Defines dataset paths, model parameters, feature dimensions, and YOLO keypoint mappings. Creates directories for output storage.\n",
    "\n",
    "**Key Configurations:**\n",
    "- Dataset paths (RWF-2000)\n",
    "- Model architecture parameters\n",
    "- Feature dimensions (10 total)\n",
    "- YOLO keypoint indices\n",
    "- Directory structure setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b47a764-2a15-46d8-9bad-fb8e7a50aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration complete\n",
      "Feature dimension: 10\n",
      "Features: ['Left Elbow Angle', 'Right Elbow Angle', 'Shoulder Angle', 'Thorax-Pelvis Rotation', 'Delta Left Elbow', 'Delta Right Elbow', 'Delta Shoulder', 'Delta Thorax-Pelvis', 'Interpersonal Distance', 'Movement Velocity']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset paths\n",
    "FIGHT_PATH = r\"D:\\\\RWF-2000 Dataset\\\\RWF2000\\\\RWF-2000\\\\train\\\\Fight\"\n",
    "NONFIGHT_PATH = r\"D:\\\\RWF-2000 Dataset\\\\RWF2000\\\\RWF-2000\\\\train\\\\NonFight\"\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATH = \"models/violence_detection_slr_aligned.keras\"\n",
    "FEATURES_PATH = \"extracted_features_slr_aligned\"\n",
    "YOLO_MODEL = \"yolov8l-pose.pt\"\n",
    "\n",
    "# Training parameters\n",
    "GPU_BATCH_SIZE = 32\n",
    "MIN_WINDOW_SIZE = 60  # SLR recommendation: 60-90 frames\n",
    "MAX_WINDOW_SIZE = 90\n",
    "CONFIDENCE_THRESHOLD = 0.25\n",
    "TRACKING_THRESHOLD = 0.7\n",
    "\n",
    "# Feature dimensions (SLR-aligned)\n",
    "ANGLE_FEATURES = 4      # Elbow angles (L/R), shoulder, thorax-pelvis\n",
    "DELTA_FEATURES = 4      # Temporal changes in angles\n",
    "SPATIAL_FEATURES = 2    # Interpersonal distance, movement velocity\n",
    "ROM_FEATURES = 2        # Range of motion metrics\n",
    "FEATURE_DIM = 10        # Total: 4 angles + 4 deltas + 2 spatial\n",
    "\n",
    "# Model architecture\n",
    "LSTM_UNITS_1 = 128\n",
    "LSTM_UNITS_2 = 64\n",
    "LSTM_UNITS_3 = 32\n",
    "DENSE_UNITS = 128\n",
    "DROPOUT_RATE = 0.4\n",
    "SPATIAL_DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.0003\n",
    "EARLY_STOPPING_PATIENCE = 25\n",
    "\n",
    "# Temporal parameters\n",
    "SHORT_TERM_WINDOW = 30\n",
    "TEMPORAL_BUFFER_SIZE = 10\n",
    "\n",
    "# Feature names for interpretability\n",
    "FEATURE_NAMES = [\n",
    "    'Left Elbow Angle', \n",
    "    'Right Elbow Angle', \n",
    "    'Shoulder Angle', \n",
    "    'Thorax-Pelvis Rotation',\n",
    "    'Delta Left Elbow', \n",
    "    'Delta Right Elbow', \n",
    "    'Delta Shoulder', \n",
    "    'Delta Thorax-Pelvis',\n",
    "    'Interpersonal Distance',\n",
    "    'Movement Velocity'\n",
    "]\n",
    "\n",
    "# YOLO keypoint indices\n",
    "KEYPOINT_INDICES = {\n",
    "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
    "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
    "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
    "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for directory in ['models', FEATURES_PATH, 'results', 'logs', 'visualizations']:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"Configuration complete\")\n",
    "print(f\"Feature dimension: {FEATURE_DIM}\")\n",
    "print(f\"Features: {FEATURE_NAMES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2163ca5-8191-4ad4-970e-a269c18628c8",
   "metadata": {},
   "source": [
    "# Section 4: Feature Extraction Class\n",
    "\n",
    "Adds multi-person pose estimation with YOLOv8 and extracts biomechanical features for violence detection.\n",
    "\n",
    "**Core Components:**\n",
    "- YOLOv8 pose estimation with BoTSORT tracking\n",
    "- Biomechanical angle computation (elbow, shoulder, thorax-pelvis)\n",
    "- Temporal delta features and spatial metrics\n",
    "- Violence potential scoring based on SLR heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa13a879-dc9e-4e43-a36a-95b325451d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class SLRAlignedExtractor:\n",
    "    \"\"\"\n",
    "    Feature extractor implementing SLR-aligned methodology:\n",
    "    - Multi-person pose estimation with YOLOv8\n",
    "    - Biomechanical angle computation\n",
    "    - Temporal delta features\n",
    "    - Spatial interaction metrics\n",
    "    - BoTSORT-inspired tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(f\"Initializing YOLO model on device: {YOLO_DEVICE}\")\n",
    "        self.pose_model = YOLO(YOLO_MODEL)\n",
    "        self.previous_angles = None\n",
    "        self.keypoint_history = []\n",
    "        self.tracked_persons = {}\n",
    "        self.next_person_id = 0\n",
    "        self.rom_cycles = defaultdict(list)\n",
    "        self.temporal_scores = deque(maxlen=SHORT_TERM_WINDOW)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Geometric Computation Methods\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def compute_angle(self, a, b, c):\n",
    "        \"\"\"\n",
    "        Calculate angle between three points using law of cosines.\n",
    "        \n",
    "        Args:\n",
    "            a, b, c: Points as (x, y) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            Angle in degrees at point b\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if (a is None or b is None or c is None or \n",
    "                np.isnan(a).any() or np.isnan(b).any() or np.isnan(c).any()):\n",
    "                return 0.0\n",
    "            \n",
    "            ba = np.array(a) - np.array(b)\n",
    "            bc = np.array(c) - np.array(b)\n",
    "            \n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
    "            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
    "            angle = np.degrees(np.arccos(cosine_angle))\n",
    "            return angle\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_thorax_pelvis_rotation(self, keypoints):\n",
    "        \"\"\"\n",
    "        Calculate thorax-pelvis rotation angle.\n",
    "        Important for detecting twisting motions in violence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not (self.is_valid_keypoint(keypoints, 5) and \n",
    "                   self.is_valid_keypoint(keypoints, 6) and\n",
    "                   self.is_valid_keypoint(keypoints, 11) and \n",
    "                   self.is_valid_keypoint(keypoints, 12)):\n",
    "                return 0.0\n",
    "            \n",
    "            thorax_vec = np.array(keypoints[6]) - np.array(keypoints[5])\n",
    "            pelvis_vec = np.array(keypoints[12]) - np.array(keypoints[11])\n",
    "            \n",
    "            cosine_angle = np.dot(thorax_vec, pelvis_vec) / \\\n",
    "                          (np.linalg.norm(thorax_vec) * np.linalg.norm(pelvis_vec) + 1e-8)\n",
    "            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
    "            angle = np.degrees(np.arccos(cosine_angle))\n",
    "            \n",
    "            return angle\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_spatial_metrics(self, keypoints_list):\n",
    "        \"\"\"\n",
    "        Calculate spatial interaction metrics between persons.\n",
    "        Returns interpersonal distance and movement velocity.\n",
    "        \"\"\"\n",
    "        if len(keypoints_list) < 2:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Interpersonal distance\n",
    "        person1, person2 = keypoints_list[0], keypoints_list[1]\n",
    "        shoulder_distances = []\n",
    "        \n",
    "        for i in [5, 6]:  # shoulder indices\n",
    "            if (self.is_valid_keypoint(person1, i) and \n",
    "                self.is_valid_keypoint(person2, i)):\n",
    "                dist = euclidean(person1[i], person2[i])\n",
    "                shoulder_distances.append(dist)\n",
    "        \n",
    "        interpersonal_distance = np.mean(shoulder_distances) if shoulder_distances else 0.0\n",
    "        \n",
    "        # Movement velocity\n",
    "        movement_velocity = 0.0\n",
    "        if len(self.keypoint_history) >= 2:\n",
    "            current_centroid = self.compute_centroid(keypoints_list[0])\n",
    "            previous_centroid = self.compute_centroid(\n",
    "                self.keypoint_history[-2][0] if len(self.keypoint_history) >= 2 \n",
    "                else keypoints_list[0]\n",
    "            )\n",
    "            if current_centroid is not None and previous_centroid is not None:\n",
    "                movement_velocity = euclidean(current_centroid, previous_centroid)\n",
    "        \n",
    "        return interpersonal_distance, movement_velocity\n",
    "    \n",
    "    def compute_centroid(self, keypoints):\n",
    "        \"\"\"Calculate centroid of valid keypoints.\"\"\"\n",
    "        valid_points = [kp for kp in keypoints \n",
    "                       if kp is not None and not np.isnan(kp).any()]\n",
    "        return np.mean(valid_points, axis=0) if valid_points else None\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Tracking Methods\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def bot_sort_tracking(self, current_detections, current_features):\n",
    "        \"\"\"\n",
    "        Simplified BoTSORT-like multi-person tracking.\n",
    "        Matches persons across frames using IoU and feature similarity.\n",
    "        \"\"\"\n",
    "        matched_persons = {}\n",
    "        \n",
    "        for detection_idx, (bbox, features) in enumerate(zip(current_detections, current_features)):\n",
    "            best_match_id = None\n",
    "            best_similarity = -1\n",
    "            \n",
    "            for person_id, person_data in self.tracked_persons.items():\n",
    "                iou = self.calculate_iou(bbox, person_data['bbox'])\n",
    "                feature_sim = self.calculate_feature_similarity(\n",
    "                    features, person_data['features']\n",
    "                )\n",
    "                \n",
    "                similarity = 0.7 * iou + 0.3 * feature_sim\n",
    "                \n",
    "                if similarity > best_similarity and similarity > TRACKING_THRESHOLD:\n",
    "                    best_similarity = similarity\n",
    "                    best_match_id = person_id\n",
    "            \n",
    "            if best_match_id is not None:\n",
    "                matched_persons[best_match_id] = {\n",
    "                    'bbox': bbox,\n",
    "                    'features': features,\n",
    "                    'age': self.tracked_persons[best_match_id]['age'] + 1\n",
    "                }\n",
    "            else:\n",
    "                new_id = self.next_person_id\n",
    "                matched_persons[new_id] = {\n",
    "                    'bbox': bbox,\n",
    "                    'features': features,\n",
    "                    'age': 1\n",
    "                }\n",
    "                self.next_person_id += 1\n",
    "        \n",
    "        self.tracked_persons = matched_persons\n",
    "        return list(matched_persons.keys())\n",
    "    \n",
    "    def calculate_iou(self, bbox1, bbox2):\n",
    "        \"\"\"Calculate Intersection over Union for bounding boxes.\"\"\"\n",
    "        try:\n",
    "            x1 = max(bbox1[0], bbox2[0])\n",
    "            y1 = max(bbox1[1], bbox2[1])\n",
    "            x2 = min(bbox1[2], bbox2[2])\n",
    "            y2 = min(bbox1[3], bbox2[3])\n",
    "            \n",
    "            intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "            area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "            area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "            \n",
    "            union = area1 + area2 - intersection\n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_feature_similarity(self, features1, features2):\n",
    "        \"\"\"Calculate normalized feature similarity.\"\"\"\n",
    "        try:\n",
    "            if len(features1) != len(features2):\n",
    "                return 0.0\n",
    "            return 1.0 - np.linalg.norm(np.array(features1) - np.array(features2)) / len(features1)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Feature Extraction Methods\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def extract_slr_features_enhanced(self, keypoints):\n",
    "        \"\"\"\n",
    "        Extract 8 base biomechanical features:\n",
    "        - 4 angle features (elbow L/R, shoulder, thorax-pelvis)\n",
    "        - 4 temporal delta features\n",
    "        \"\"\"\n",
    "        features = [0.0] * 8\n",
    "        \n",
    "        try:\n",
    "            # Feature 1: Left elbow angle\n",
    "            if (self.is_valid_keypoint(keypoints, 5) and \n",
    "                self.is_valid_keypoint(keypoints, 7) and \n",
    "                self.is_valid_keypoint(keypoints, 9)):\n",
    "                features[0] = self.compute_angle(\n",
    "                    keypoints[5], keypoints[7], keypoints[9]\n",
    "                )\n",
    "            \n",
    "            # Feature 2: Right elbow angle\n",
    "            if (self.is_valid_keypoint(keypoints, 6) and \n",
    "                self.is_valid_keypoint(keypoints, 8) and \n",
    "                self.is_valid_keypoint(keypoints, 10)):\n",
    "                features[1] = self.compute_angle(\n",
    "                    keypoints[6], keypoints[8], keypoints[10]\n",
    "                )\n",
    "            \n",
    "            # Feature 3: Shoulder angle\n",
    "            if (self.is_valid_keypoint(keypoints, 5) and \n",
    "                self.is_valid_keypoint(keypoints, 6)):\n",
    "                shoulder_vec = np.array(keypoints[6]) - np.array(keypoints[5])\n",
    "                horizontal_vec = np.array([1, 0])\n",
    "                cosine_angle = np.dot(shoulder_vec, horizontal_vec) / \\\n",
    "                              (np.linalg.norm(shoulder_vec) + 1e-8)\n",
    "                cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
    "                features[2] = np.degrees(np.arccos(cosine_angle))\n",
    "            \n",
    "            # Feature 4: Thorax-pelvis rotation\n",
    "            features[3] = self.compute_thorax_pelvis_rotation(keypoints)\n",
    "            \n",
    "            # Features 5-8: Temporal deltas\n",
    "            if self.previous_angles is not None:\n",
    "                for i in range(4):\n",
    "                    features[4 + i] = features[i] - self.previous_angles[i]\n",
    "            \n",
    "            self.previous_angles = features[:4].copy()\n",
    "            \n",
    "            self.keypoint_history.append(keypoints)\n",
    "            if len(self.keypoint_history) > 5:\n",
    "                self.keypoint_history.pop(0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Feature extraction error: {e}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def is_valid_keypoint(self, keypoints, index):\n",
    "        \"\"\"Check if a keypoint is valid and visible.\"\"\"\n",
    "        try:\n",
    "            if (keypoints is None or index >= len(keypoints) or \n",
    "                keypoints[index] is None or np.isnan(keypoints[index]).any()):\n",
    "                return False\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_frame_features_enhanced(self, frame):\n",
    "        \"\"\"\n",
    "        Main feature extraction pipeline for a single frame.\n",
    "        Returns 10-dimensional feature vector.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Run YOLO pose detection\n",
    "            results = self.pose_model(\n",
    "                frame, conf=CONFIDENCE_THRESHOLD, verbose=False, device=YOLO_DEVICE\n",
    "            )\n",
    "            \n",
    "            if not results or len(results) == 0:\n",
    "                return [0.0] * FEATURE_DIM\n",
    "            \n",
    "            result = results[0]\n",
    "            \n",
    "            if not hasattr(result, 'keypoints') or result.keypoints is None:\n",
    "                return [0.0] * FEATURE_DIM\n",
    "            \n",
    "            try:\n",
    "                keypoints_data = result.keypoints.data\n",
    "                if keypoints_data is None or len(keypoints_data) == 0:\n",
    "                    return [0.0] * FEATURE_DIM\n",
    "            except:\n",
    "                return [0.0] * FEATURE_DIM\n",
    "            \n",
    "            all_person_features = []\n",
    "            all_keypoints = []\n",
    "            bboxes = []\n",
    "            \n",
    "            # Process each detected person\n",
    "            for person_idx in range(len(keypoints_data)):\n",
    "                try:\n",
    "                    person_keypoints = keypoints_data[person_idx]\n",
    "                    \n",
    "                    if hasattr(person_keypoints, 'cpu'):\n",
    "                        keypoints_np = person_keypoints.cpu().numpy()\n",
    "                    else:\n",
    "                        keypoints_np = person_keypoints\n",
    "                    \n",
    "                    if hasattr(result.keypoints, 'conf'):\n",
    "                        confidences = result.keypoints.conf[person_idx].cpu().numpy() \\\n",
    "                                     if hasattr(result.keypoints.conf[person_idx], 'cpu') \\\n",
    "                                     else result.keypoints.conf[person_idx]\n",
    "                    else:\n",
    "                        confidences = np.ones(len(keypoints_np))\n",
    "                    \n",
    "                    if (hasattr(result, 'boxes') and result.boxes is not None and \n",
    "                        len(result.boxes) > person_idx):\n",
    "                        bbox = result.boxes.xyxy[person_idx].cpu().numpy() \\\n",
    "                              if hasattr(result.boxes.xyxy[person_idx], 'cpu') \\\n",
    "                              else result.boxes.xyxy[person_idx]\n",
    "                    else:\n",
    "                        bbox = [0, 0, 1, 1]\n",
    "                    \n",
    "                    # Validate keypoints\n",
    "                    validated_keypoints = []\n",
    "                    for i in range(len(keypoints_np)):\n",
    "                        kp = keypoints_np[i]\n",
    "                        conf = confidences[i] if i < len(confidences) else 1.0\n",
    "                        \n",
    "                        if (kp is not None and len(kp) >= 2 and \n",
    "                            not np.isnan(kp).any() and conf > CONFIDENCE_THRESHOLD):\n",
    "                            validated_keypoints.append((float(kp[0]), float(kp[1])))\n",
    "                        else:\n",
    "                            validated_keypoints.append(None)\n",
    "                    \n",
    "                    features = self.extract_slr_features_enhanced(validated_keypoints)\n",
    "                    \n",
    "                    if len(features) < 8:\n",
    "                        features.extend([0.0] * (8 - len(features)))\n",
    "                    elif len(features) > 8:\n",
    "                        features = features[:8]\n",
    "                    \n",
    "                    all_person_features.append(features)\n",
    "                    all_keypoints.append(validated_keypoints)\n",
    "                    bboxes.append(bbox)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if not all_person_features:\n",
    "                return [0.0] * FEATURE_DIM\n",
    "            \n",
    "            # Multi-person tracking\n",
    "            if len(all_person_features) > 1:\n",
    "                try:\n",
    "                    person_ids = self.bot_sort_tracking(bboxes, all_person_features)\n",
    "                except:\n",
    "                    person_ids = list(range(len(all_person_features)))\n",
    "            else:\n",
    "                person_ids = [0]\n",
    "            \n",
    "            # Spatial metrics\n",
    "            try:\n",
    "                if len(all_keypoints) >= 2:\n",
    "                    spatial_metrics = self.compute_spatial_metrics(all_keypoints)\n",
    "                else:\n",
    "                    spatial_metrics = (0.0, 0.0)\n",
    "            except:\n",
    "                spatial_metrics = (0.0, 0.0)\n",
    "            \n",
    "            person_violence_scores = []\n",
    "            final_features_list = []\n",
    "            \n",
    "            for i, (features, person_id) in enumerate(zip(all_person_features, person_ids)):\n",
    "                try:\n",
    "                    enhanced_features = features + list(spatial_metrics)\n",
    "                    \n",
    "                    if len(enhanced_features) < FEATURE_DIM:\n",
    "                        enhanced_features.extend([0.0] * (FEATURE_DIM - len(enhanced_features)))\n",
    "                    \n",
    "                    violence_score = self.calculate_violence_potential_slr(\n",
    "                        enhanced_features, person_id\n",
    "                    )\n",
    "                    person_violence_scores.append(violence_score)\n",
    "                    final_features_list.append(enhanced_features)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if final_features_list:\n",
    "                if len(final_features_list) == 1:\n",
    "                    return final_features_list[0]\n",
    "                else:\n",
    "                    try:\n",
    "                        max_violence_idx = np.argmax(person_violence_scores)\n",
    "                        return final_features_list[max_violence_idx]\n",
    "                    except:\n",
    "                        return final_features_list[0]\n",
    "            else:\n",
    "                return [0.0] * FEATURE_DIM\n",
    "                \n",
    "        except Exception as e:\n",
    "            return [0.0] * FEATURE_DIM\n",
    "    \n",
    "    def calculate_violence_potential_slr(self, features, person_id):\n",
    "        \"\"\"\n",
    "        Calculate violence potential score using SLR-inspired heuristics.\n",
    "        \"\"\"\n",
    "        base_score = 0.0\n",
    "        \n",
    "        try:\n",
    "            angles = features[:4]\n",
    "            deltas = features[4:8]\n",
    "            spatial = features[8:] if len(features) > 8 else [0.0, 0.0]\n",
    "            \n",
    "            # Extreme angle detection\n",
    "            extreme_angles = sum(1 for angle in angles if angle > 120 or angle < 30)\n",
    "            base_score += extreme_angles * 0.25\n",
    "            \n",
    "            # Rapid movement detection\n",
    "            rapid_movements = sum(1 for delta in deltas if abs(delta) > 5)\n",
    "            base_score += rapid_movements * 0.35\n",
    "            \n",
    "            # Thorax-pelvis rotation\n",
    "            if angles[3] > 45:\n",
    "                base_score += 0.3\n",
    "            \n",
    "            # Spatial aggression indicators\n",
    "            if spatial[0] > 0 and spatial[0] < 0.1:\n",
    "                base_score += 0.2\n",
    "            if spatial[1] > 0.05:\n",
    "                base_score += 0.15\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Temporal smoothing\n",
    "        self.temporal_scores.append(base_score)\n",
    "        if len(self.temporal_scores) >= SHORT_TERM_WINDOW:\n",
    "            smoothed_score = max(self.temporal_scores)\n",
    "        else:\n",
    "            smoothed_score = base_score\n",
    "        \n",
    "        return min(smoothed_score, 1.0)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Video Processing Methods\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def process_video_enhanced(self, video_path, max_frames=None):\n",
    "        \"\"\"\n",
    "        Process entire video and extract feature windows.\n",
    "        \"\"\"\n",
    "        print(f\"Processing: {os.path.basename(video_path)}\")\n",
    "        \n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"  Cannot open video: {video_path}\")\n",
    "                return []\n",
    "            \n",
    "            original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if total_frames == 0 or original_fps == 0:\n",
    "                print(f\"  Invalid video: {video_path}\")\n",
    "                cap.release()\n",
    "                return []\n",
    "            \n",
    "            # Adaptive window sizing\n",
    "            if original_fps < 20:\n",
    "                window_size = 60\n",
    "                stride = 30\n",
    "            else:\n",
    "                window_size = 75\n",
    "                stride = 38\n",
    "            \n",
    "            print(f\"  FPS={original_fps:.1f}, Window={window_size}, Stride={stride}\")\n",
    "            \n",
    "            video_features = []\n",
    "            frame_count = 0\n",
    "            \n",
    "            # Reset tracking\n",
    "            self.previous_angles = None\n",
    "            self.keypoint_history = []\n",
    "            self.tracked_persons = {}\n",
    "            self.next_person_id = 0\n",
    "            self.rom_cycles.clear()\n",
    "            self.temporal_scores.clear()\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or (max_frames and frame_count >= max_frames):\n",
    "                    break\n",
    "                \n",
    "                features = self.extract_frame_features_enhanced(frame)\n",
    "                video_features.append(features)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count % 50 == 0:\n",
    "                    print(f\"    Processed {frame_count}/{total_frames} frames\")\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Feature smoothing\n",
    "            if len(video_features) >= 11:\n",
    "                try:\n",
    "                    video_features = self.smooth_features(video_features)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Create windows\n",
    "            video_windows = self.create_windows(video_features, window_size, stride)\n",
    "            print(f\"  Extracted {len(video_windows)} windows from {len(video_features)} frames\")\n",
    "            return video_windows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {video_path}: {e}\")\n",
    "            try:\n",
    "                cap.release()\n",
    "            except:\n",
    "                pass\n",
    "            return []\n",
    "    \n",
    "    def smooth_features(self, features):\n",
    "        \"\"\"Apply Savitzky-Golay filter for temporal smoothing.\"\"\"\n",
    "        try:\n",
    "            features_array = np.array(features)\n",
    "            smoothed = np.zeros_like(features_array)\n",
    "            \n",
    "            for i in range(FEATURE_DIM):\n",
    "                try:\n",
    "                    smoothed[:, i] = savgol_filter(\n",
    "                        features_array[:, i], window_length=11, polyorder=3\n",
    "                    )\n",
    "                except:\n",
    "                    smoothed[:, i] = features_array[:, i]\n",
    "            \n",
    "            return smoothed.tolist()\n",
    "        except:\n",
    "            return features\n",
    "    \n",
    "    def create_windows(self, video_features, window_size, stride):\n",
    "        \"\"\"Create sliding temporal windows.\"\"\"\n",
    "        try:\n",
    "            if len(video_features) < window_size:\n",
    "                if len(video_features) >= 30:\n",
    "                    window = video_features\n",
    "                    if len(window) < window_size:\n",
    "                        padding = [[0.0] * FEATURE_DIM] * (window_size - len(window))\n",
    "                        window.extend(padding)\n",
    "                    return [window]\n",
    "                else:\n",
    "                    return []\n",
    "            \n",
    "            windows = []\n",
    "            for start_idx in range(0, len(video_features) - window_size + 1, stride):\n",
    "                window = video_features[start_idx:start_idx + window_size]\n",
    "                windows.append(window)\n",
    "            \n",
    "            return windows\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "print(\"Feature extractor initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265cff8-b2dc-453d-b1fc-962998032d1b",
   "metadata": {},
   "source": [
    "# Section 5: Training System\n",
    "## Complete Violence Detection Pipeline\n",
    "\n",
    "Create end-to-end training with feature extraction, dataset preparation, model architecture, and evaluation.\n",
    "\n",
    "**Core Pipeline:**\n",
    "- Feature extraction from RWF-2000 videos\n",
    "- Dataset balancing with augmentation\n",
    "- BiLSTM model with attention mechanism\n",
    "- Comprehensive evaluation metrics\n",
    "- Model saving and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3693564c-d7bb-4c1e-aa66-31716a3d165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TRAINING SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class SLRAlignedViolenceDetection:\n",
    "    \"\"\"\n",
    "    Complete training pipeline for violence detection:\n",
    "    - Feature extraction from videos\n",
    "    - Dataset preparation with augmentation\n",
    "    - Model training with BiLSTM + attention\n",
    "    - Evaluation with comprehensive metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.extractor = SLRAlignedExtractor()\n",
    "        self.results_dir = \"slr_aligned_results\"\n",
    "        self.visualization_dir = \"visualizations\"\n",
    "        self.scaler_path = os.path.join(FEATURES_PATH, \"slr_aligned_scaler.pkl\")\n",
    "        self.scaler = None\n",
    "        self.class_weights = None\n",
    "\n",
    "        for directory in [self.results_dir, self.visualization_dir, FEATURES_PATH]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Feature Extraction\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def extract_features(self, fight_path, nonfight_path, \n",
    "                        max_videos_per_class=None, max_frames_per_video=None):\n",
    "        \"\"\"\n",
    "        Extract features from fight and non-fight videos.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"FEATURE EXTRACTION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Process fight videos\n",
    "        fight_features = []\n",
    "        fight_videos = [f for f in os.listdir(fight_path) \n",
    "                       if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "        if max_videos_per_class:\n",
    "            fight_videos = fight_videos[:max_videos_per_class]\n",
    "\n",
    "        print(f\"Processing {len(fight_videos)} FIGHT videos...\")\n",
    "        for i, video_file in enumerate(fight_videos):\n",
    "            print(f\"  [{i+1}/{len(fight_videos)}] {video_file}\")\n",
    "            video_path = os.path.join(fight_path, video_file)\n",
    "            windows = self.extractor.process_video_enhanced(\n",
    "                video_path, max_frames=max_frames_per_video\n",
    "            )\n",
    "            fight_features.extend(windows)\n",
    "\n",
    "        # Process non-fight videos\n",
    "        nonfight_features = []\n",
    "        nonfight_videos = [f for f in os.listdir(nonfight_path) \n",
    "                          if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "        if max_videos_per_class:\n",
    "            nonfight_videos = nonfight_videos[:max_videos_per_class]\n",
    "\n",
    "        print(f\"Processing {len(nonfight_videos)} NON-FIGHT videos...\")\n",
    "        for i, video_file in enumerate(nonfight_videos):\n",
    "            print(f\"  [{i+1}/{len(nonfight_videos)}] {video_file}\")\n",
    "            video_path = os.path.join(nonfight_path, video_file)\n",
    "            windows = self.extractor.process_video_enhanced(\n",
    "                video_path, max_frames=max_frames_per_video\n",
    "            )\n",
    "            nonfight_features.extend(windows)\n",
    "\n",
    "        # Combine features and labels\n",
    "        all_features = fight_features + nonfight_features\n",
    "        all_labels = [1] * len(fight_features) + [0] * len(nonfight_features)\n",
    "\n",
    "        print(f\"\\nFeature Summary:\")\n",
    "        print(f\"  Fight samples: {len(fight_features)}\")\n",
    "        print(f\"  Non-Fight samples: {len(nonfight_features)}\")\n",
    "        print(f\"  Total samples: {len(all_features)}\")\n",
    "        print(f\"  Feature dimension: {FEATURE_DIM}\")\n",
    "\n",
    "        if len(all_features) == 0:\n",
    "            print(\"ERROR: No features extracted!\")\n",
    "            return None, None\n",
    "\n",
    "        # Save features\n",
    "        np.save(\n",
    "            os.path.join(FEATURES_PATH, 'slr_aligned_features.npy'), \n",
    "            np.array(all_features, dtype=np.float32)\n",
    "        )\n",
    "        np.save(\n",
    "            os.path.join(FEATURES_PATH, 'slr_aligned_labels.npy'), \n",
    "            np.array(all_labels, dtype=np.int32)\n",
    "        )\n",
    "\n",
    "        print(f\"Features saved to {FEATURES_PATH}/\")\n",
    "        return all_features, all_labels\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Dataset Preparation\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def prepare_dataset(self, test_size=0.2, validation_size=0.15, \n",
    "                       balance=True, random_state=42):\n",
    "        \"\"\"\n",
    "        Prepare train/validation/test splits with normalization and balancing.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DATASET PREPARATION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        features_path = os.path.join(FEATURES_PATH, 'slr_aligned_features.npy')\n",
    "        labels_path = os.path.join(FEATURES_PATH, 'slr_aligned_labels.npy')\n",
    "\n",
    "        if not (os.path.exists(features_path) and os.path.exists(labels_path)):\n",
    "            print(\"ERROR: No features found! Run extraction first.\")\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "        X = np.load(features_path)\n",
    "        y = np.load(labels_path)\n",
    "\n",
    "        print(f\"Loaded {len(X)} samples\")\n",
    "        print(f\"Class distribution - Class 0: {np.sum(y == 0)}, Class 1: {np.sum(y == 1)}\")\n",
    "\n",
    "        # Train/test split\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, \n",
    "            stratify=y, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Train/validation split\n",
    "        val_size_adjusted = validation_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, \n",
    "            stratify=y_temp, shuffle=True\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSplit sizes:\")\n",
    "        print(f\"  Train: {len(y_train)} (0:{np.sum(y_train==0)}, 1:{np.sum(y_train==1)})\")\n",
    "        print(f\"  Val:   {len(y_val)} (0:{np.sum(y_val==0)}, 1:{np.sum(y_val==1)})\")\n",
    "        print(f\"  Test:  {len(y_test)} (0:{np.sum(y_test==0)}, 1:{np.sum(y_test==1)})\")\n",
    "\n",
    "        # Normalize features\n",
    "        self.scaler = RobustScaler()\n",
    "        X_train_flat = X_train.reshape(-1, FEATURE_DIM)\n",
    "        X_train_flat_norm = self.scaler.fit_transform(X_train_flat)\n",
    "        X_train_norm = X_train_flat_norm.reshape(X_train.shape)\n",
    "\n",
    "        X_val_flat = X_val.reshape(-1, FEATURE_DIM)\n",
    "        X_val_flat_norm = self.scaler.transform(X_val_flat)\n",
    "        X_val_norm = X_val_flat_norm.reshape(X_val.shape)\n",
    "\n",
    "        X_test_flat = X_test.reshape(-1, FEATURE_DIM)\n",
    "        X_test_flat_norm = self.scaler.transform(X_test_flat)\n",
    "        X_test_norm = X_test_flat_norm.reshape(X_test.shape)\n",
    "\n",
    "        # Save scaler\n",
    "        with open(self.scaler_path, \"wb\") as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        print(f\"Scaler saved to {self.scaler_path}\")\n",
    "\n",
    "        # Balance dataset with augmentation\n",
    "        if balance:\n",
    "            X_train_norm, y_train = self._balance_with_augmentation(X_train_norm, y_train)\n",
    "\n",
    "        # Calculate class weights\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "        self.class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "        print(f\"Class weights: {self.class_weights}\")\n",
    "\n",
    "        # Save processed data\n",
    "        np.save(os.path.join(FEATURES_PATH, 'X_train_slr_aligned.npy'), X_train_norm)\n",
    "        np.save(os.path.join(FEATURES_PATH, 'X_val_slr_aligned.npy'), X_val_norm)\n",
    "        np.save(os.path.join(FEATURES_PATH, 'X_test_slr_aligned.npy'), X_test_norm)\n",
    "        np.save(os.path.join(FEATURES_PATH, 'y_train_slr_aligned.npy'), y_train)\n",
    "        np.save(os.path.join(FEATURES_PATH, 'y_val_slr_aligned.npy'), y_val)\n",
    "        np.save(os.path.join(FEATURES_PATH, 'y_test_slr_aligned.npy'), y_test)\n",
    "\n",
    "        print(f\"\\nFinal shapes - Train: {X_train_norm.shape}, Val: {X_val_norm.shape}, Test: {X_test_norm.shape}\")\n",
    "        return X_train_norm, X_val_norm, X_test_norm, y_train, y_val, y_test\n",
    "    \n",
    "    def _balance_with_augmentation(self, X, y):\n",
    "        \"\"\"Balance dataset using data augmentation techniques.\"\"\"\n",
    "        pos_idx = np.where(y == 1)[0]\n",
    "        neg_idx = np.where(y == 0)[0]\n",
    "        n_pos, n_neg = len(pos_idx), len(neg_idx)\n",
    "        \n",
    "        print(f\"Before balancing - Pos: {n_pos}, Neg: {n_neg}\")\n",
    "        \n",
    "        if n_pos == 0 or n_neg == 0:\n",
    "            return X, y\n",
    "        \n",
    "        # Determine minority class\n",
    "        if n_pos < n_neg:\n",
    "            minority_idx = pos_idx\n",
    "            samples_needed = n_neg - n_pos\n",
    "        else:\n",
    "            minority_idx = neg_idx\n",
    "            samples_needed = n_pos - n_neg\n",
    "        \n",
    "        augmented_X = []\n",
    "        augmented_y = []\n",
    "        \n",
    "        # Generate augmented samples\n",
    "        for _ in range(samples_needed):\n",
    "            idx = np.random.choice(minority_idx)\n",
    "            sample = X[idx].copy()\n",
    "            \n",
    "            aug_type = np.random.randint(0, 4)\n",
    "            \n",
    "            if aug_type == 0:  # Add noise\n",
    "                noise = np.random.normal(0, 0.03, sample.shape)\n",
    "                sample = sample + noise\n",
    "            elif aug_type == 1:  # Time shift\n",
    "                shift = np.random.randint(-3, 4)\n",
    "                sample = np.roll(sample, shift, axis=0)\n",
    "            elif aug_type == 2:  # Scaling\n",
    "                scale = np.random.uniform(0.95, 1.05)\n",
    "                sample = sample * scale\n",
    "            else:  # Time stretching\n",
    "                indices = np.arange(len(sample))\n",
    "                stretched = np.linspace(\n",
    "                    0, len(sample)-1, int(len(sample)*np.random.uniform(0.95, 1.05))\n",
    "                )\n",
    "                for feat_idx in range(sample.shape[1]):\n",
    "                    sample[:, feat_idx] = np.interp(\n",
    "                        indices, stretched, \n",
    "                        np.interp(stretched, indices, sample[:, feat_idx])\n",
    "                    )\n",
    "            \n",
    "            augmented_X.append(sample)\n",
    "            augmented_y.append(y[idx])\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        X_balanced = np.concatenate([X, np.array(augmented_X)], axis=0)\n",
    "        y_balanced = np.concatenate([y, np.array(augmented_y)], axis=0)\n",
    "        \n",
    "        shuffle_idx = np.random.permutation(len(X_balanced))\n",
    "        X_balanced = X_balanced[shuffle_idx]\n",
    "        y_balanced = y_balanced[shuffle_idx]\n",
    "        \n",
    "        print(f\"After balancing - Total: {len(X_balanced)}, Pos: {np.sum(y_balanced==1)}, Neg: {np.sum(y_balanced==0)}\")\n",
    "        return X_balanced, y_balanced\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Model Architecture\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def build_improved_model(self, input_shape, initial_bias=None):\n",
    "        \"\"\"\n",
    "        Build BiLSTM model with attention mechanism.\n",
    "        Simplified architecture with strong regularization.\n",
    "        \"\"\"\n",
    "        output_bias = tf.keras.initializers.Constant(initial_bias) \\\n",
    "                     if initial_bias is not None else 'zeros'\n",
    "\n",
    "        inputs = layers.Input(shape=input_shape, name='input_layer')\n",
    "        \n",
    "        # First BiLSTM layer\n",
    "        x = layers.Bidirectional(\n",
    "            layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)\n",
    "        )(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Second BiLSTM layer\n",
    "        x = layers.Bidirectional(\n",
    "            layers.LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = layers.Dense(64, activation='relu', \n",
    "                        kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu', \n",
    "                        kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(1, activation='sigmoid', \n",
    "                              bias_initializer=output_bias, name='output')(x)\n",
    "        \n",
    "        model = models.Model(inputs=inputs, outputs=outputs, \n",
    "                           name='SLRViolenceDetector')\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.0001, clipnorm=1.0),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                tf.keras.metrics.AUC(name='auc', curve='ROC'),\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='pr_auc', curve='PR')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"\\nMODEL ARCHITECTURE\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Input shape: {input_shape}\")\n",
    "        print(f\"Feature dimension: {FEATURE_DIM}\")\n",
    "        model.summary()\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Training\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, epochs=150):\n",
    "        \"\"\"\n",
    "        Train the violence detection model.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"MODEL TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"Train: Pos={np.sum(y_train==1)}, Neg={np.sum(y_train==0)}\")\n",
    "        print(f\"Val:   Pos={np.sum(y_val==1)}, Neg={np.sum(y_val==0)}\")\n",
    "    \n",
    "        # Calculate initial bias\n",
    "        pos = float(np.sum(y_train))\n",
    "        neg = float(len(y_train) - pos)\n",
    "        initial_bias = np.log((pos + 1e-7) / (neg + 1e-7)) \\\n",
    "                      if pos > 0 and neg > 0 else None\n",
    "        print(f\"Initial bias: {initial_bias:.4f}\" if initial_bias else \"No bias\")\n",
    "    \n",
    "        # Build model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        self.build_improved_model(input_shape, initial_bias=initial_bias)\n",
    "    \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_auc',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                mode='max',\n",
    "                min_delta=0.001,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=MODEL_PATH,\n",
    "                monitor='val_auc',\n",
    "                save_best_only=True,\n",
    "                mode='max',\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                os.path.join('logs', 'training_log.csv')\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "        try:\n",
    "            batch_size = min(16, GPU_BATCH_SIZE)\n",
    "            print(f\"Batch size: {batch_size}\")\n",
    "            print(\"Starting training...\")\n",
    "            \n",
    "            self.history = self.model.fit(\n",
    "                X_train, y_train.astype(np.float32),\n",
    "                validation_data=(X_val, y_val.astype(np.float32)),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                class_weight=None,\n",
    "                callbacks=callbacks,\n",
    "                verbose=2,\n",
    "                shuffle=True\n",
    "            )\n",
    "    \n",
    "            self.model.save(MODEL_PATH)\n",
    "            print(f\"\\nModel saved to: {MODEL_PATH}\")\n",
    "            \n",
    "            self.plot_training_history()\n",
    "            \n",
    "            return self.history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training metrics over epochs.\"\"\"\n",
    "        try:\n",
    "            if self.history is None:\n",
    "                return\n",
    "                \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # Accuracy\n",
    "            axes[0, 0].plot(self.history.history['accuracy'], label='Train')\n",
    "            axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')\n",
    "            axes[0, 0].set_title('Model Accuracy')\n",
    "            axes[0, 0].set_xlabel('Epoch')\n",
    "            axes[0, 0].set_ylabel('Accuracy')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True)\n",
    "            \n",
    "            # Loss\n",
    "            axes[0, 1].plot(self.history.history['loss'], label='Train')\n",
    "            axes[0, 1].plot(self.history.history['val_loss'], label='Validation')\n",
    "            axes[0, 1].set_title('Model Loss')\n",
    "            axes[0, 1].set_xlabel('Epoch')\n",
    "            axes[0, 1].set_ylabel('Loss')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True)\n",
    "            \n",
    "            # AUC\n",
    "            axes[1, 0].plot(self.history.history['auc'], label='Train')\n",
    "            axes[1, 0].plot(self.history.history['val_auc'], label='Validation')\n",
    "            axes[1, 0].set_title('Model AUC')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('AUC')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True)\n",
    "            \n",
    "            # PR AUC\n",
    "            if 'pr_auc' in self.history.history:\n",
    "                axes[1, 1].plot(self.history.history['pr_auc'], label='Train')\n",
    "                axes[1, 1].plot(self.history.history['val_pr_auc'], label='Validation')\n",
    "                axes[1, 1].set_title('Precision-Recall AUC')\n",
    "                axes[1, 1].set_xlabel('Epoch')\n",
    "                axes[1, 1].set_ylabel('PR AUC')\n",
    "                axes[1, 1].legend()\n",
    "                axes[1, 1].grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                os.path.join(self.visualization_dir, 'training_history.png'), \n",
    "                dpi=300, bbox_inches='tight'\n",
    "            )\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Training history plot saved to {self.visualization_dir}/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot training history: {e}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Model Loading & Evaluation\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load trained model and scaler from disk.\"\"\"\n",
    "        if os.path.exists(MODEL_PATH):\n",
    "            try:\n",
    "                self.model = tf.keras.models.load_model(MODEL_PATH)\n",
    "                print(f\"Model loaded from: {MODEL_PATH}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(self.scaler_path, \"rb\") as f:\n",
    "                        self.scaler = pickle.load(f)\n",
    "                    print(f\"Scaler loaded from: {self.scaler_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: could not load scaler: {e}\")\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"No trained model found!\")\n",
    "            return False\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation with multiple metrics.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"MODEL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"Test set size: {len(y_test)} samples\")\n",
    "        print(f\"Class distribution - 0: {np.sum(y_test == 0)}, 1: {np.sum(y_test == 1)}\")\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(\"No model loaded!\")\n",
    "            return None\n",
    "\n",
    "        print(\"\\nRunning predictions...\")\n",
    "        try:\n",
    "            probs = self.model.predict(\n",
    "                X_test, batch_size=GPU_BATCH_SIZE, verbose=1\n",
    "            ).flatten()\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "            \n",
    "            preds = (probs > optimal_threshold).astype(int)\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            precision = precision_score(y_test, preds, zero_division=0)\n",
    "            recall = recall_score(y_test, preds, zero_division=0)\n",
    "            f1 = f1_score(y_test, preds, zero_division=0)\n",
    "            \n",
    "            if len(np.unique(y_test)) > 1:\n",
    "                roc_auc = roc_auc_score(y_test, probs)\n",
    "                avg_precision = average_precision_score(y_test, probs)\n",
    "            else:\n",
    "                roc_auc = 0.5\n",
    "                avg_precision = 0.5\n",
    "                \n",
    "            mcc = matthews_corrcoef(y_test, preds) \\\n",
    "                 if len(np.unique(preds)) > 1 else 0.0\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_test, preds)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "            # Print results\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"PERFORMANCE METRICS\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Accuracy:             {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "            print(f\"Balanced Accuracy:    {balanced_accuracy:.4f} ({balanced_accuracy*100:.2f}%)\")\n",
    "            print(f\"Precision:            {precision:.4f} ({precision*100:.2f}%)\")\n",
    "            print(f\"Recall (Sensitivity): {recall:.4f} ({recall*100:.2f}%)\")\n",
    "            print(f\"Specificity:          {specificity:.4f} ({specificity*100:.2f}%)\")\n",
    "            print(f\"F1-Score:             {f1:.4f}\")\n",
    "            print(f\"ROC AUC:              {roc_auc:.4f}\")\n",
    "            print(f\"PR AUC:               {avg_precision:.4f}\")\n",
    "            print(f\"MCC:                  {mcc:.4f}\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            # Performance assessment\n",
    "            if accuracy >= 0.85 and f1 >= 0.85:\n",
    "                print(\"\\nEXCELLENT: Target achieved (85%+)\")\n",
    "            elif accuracy >= 0.80 and f1 >= 0.80:\n",
    "                print(\"\\nGOOD: Strong performance (80%+)\")\n",
    "            elif accuracy >= 0.75:\n",
    "                print(\"\\nACCEPTABLE: Moderate performance (75%+)\")\n",
    "            else:\n",
    "                print(\"\\nNEEDS IMPROVEMENT: Below target (<75%)\")\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            self.plot_confusion_matrix(cm, accuracy)\n",
    "\n",
    "            # Save results\n",
    "            results = {\n",
    "                'accuracy': float(accuracy),\n",
    "                'balanced_accuracy': float(balanced_accuracy),\n",
    "                'precision': float(precision),\n",
    "                'recall': float(recall),\n",
    "                'specificity': float(specificity),\n",
    "                'f1_score': float(f1),\n",
    "                'roc_auc': float(roc_auc),\n",
    "                'pr_auc': float(avg_precision),\n",
    "                'mcc': float(mcc),\n",
    "                'optimal_threshold': float(optimal_threshold),\n",
    "                'confusion_matrix': cm.tolist()\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(os.path.join(self.results_dir, 'evaluation_results.json'), 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            print(f\"\\nResults saved to {self.results_dir}/\")\n",
    "\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, accuracy):\n",
    "        \"\"\"Visualize confusion matrix.\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Non-Fight', 'Fight'],\n",
    "                       yticklabels=['Non-Fight', 'Fight'])\n",
    "            plt.title(f'Confusion Matrix (Accuracy: {accuracy:.2%})')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.savefig(\n",
    "                os.path.join(self.visualization_dir, 'confusion_matrix.png'), \n",
    "                dpi=300, bbox_inches='tight'\n",
    "            )\n",
    "            plt.close()\n",
    "            print(f\"Confusion matrix saved to {self.visualization_dir}/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot confusion matrix: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61559e4d-7c76-40aa-b366-54184224ad6c",
   "metadata": {},
   "source": [
    "# Section 6: Interactive Menu System\n",
    "## User Interface for Pipeline Control\n",
    "\n",
    "**Menu Options:**\n",
    "1. Extract Features from Videos\n",
    "2. Train Model  \n",
    "3. Evaluate Model\n",
    "4. Run Full Pipeline\n",
    "5. Exit\n",
    "\n",
    "**Features:**\n",
    "- Parameter configuration via user input\n",
    "- Progress tracking for full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5240349d-af21-4509-af59-7c7cd4c7d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_menu():\n",
    "    \"\"\"\n",
    "    Interactive menu for running different pipeline stages.\n",
    "    \"\"\"\n",
    "    system = SLRAlignedViolenceDetection()\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SLR-ALIGNED VIOLENCE DETECTION SYSTEM\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"1. Extract Features from Videos\")\n",
    "        print(\"2. Train Model\")\n",
    "        print(\"3. Evaluate Model\")\n",
    "        print(\"4. Run Full Pipeline\")\n",
    "        print(\"5. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nSelect option (1-5): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            # Extract features\n",
    "            try:\n",
    "                max_videos = input(\"Max videos per class (press Enter for all): \").strip()\n",
    "                max_videos = int(max_videos) if max_videos.isdigit() else None\n",
    "                \n",
    "                max_frames = input(\"Max frames per video (press Enter for all): \").strip()\n",
    "                max_frames = int(max_frames) if max_frames.isdigit() else None\n",
    "                \n",
    "                features, labels = system.extract_features(\n",
    "                    FIGHT_PATH, NONFIGHT_PATH, \n",
    "                    max_videos_per_class=max_videos, \n",
    "                    max_frames_per_video=max_frames\n",
    "                )\n",
    "                \n",
    "                if features is not None:\n",
    "                    print(\"\\nFeature extraction completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            # Train model\n",
    "            try:\n",
    "                data = system.prepare_dataset(test_size=0.2, validation_size=0.15)\n",
    "                \n",
    "                if data[0] is None:\n",
    "                    print(\"No data available!\")\n",
    "                    continue\n",
    "                    \n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = data\n",
    "                \n",
    "                epochs = input(\"Number of epochs (press Enter for 150): \").strip()\n",
    "                epochs = int(epochs) if epochs.isdigit() else 150\n",
    "                \n",
    "                history = system.train_model(X_train, y_train, X_val, y_val, epochs=epochs)\n",
    "                \n",
    "                if history is not None:\n",
    "                    print(\"\\nTraining completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            # Evaluate model\n",
    "            try:\n",
    "                if not system.load_model():\n",
    "                    continue\n",
    "                    \n",
    "                X_test_path = os.path.join(FEATURES_PATH, 'X_test_slr_aligned.npy')\n",
    "                y_test_path = os.path.join(FEATURES_PATH, 'y_test_slr_aligned.npy')\n",
    "                \n",
    "                if not (os.path.exists(X_test_path) and os.path.exists(y_test_path)):\n",
    "                    print(\"No test data found!\")\n",
    "                    continue\n",
    "                    \n",
    "                X_test = np.load(X_test_path)\n",
    "                y_test = np.load(y_test_path)\n",
    "                \n",
    "                metrics = system.evaluate_model(X_test, y_test)\n",
    "                \n",
    "                if metrics:\n",
    "                    print(\"\\nEvaluation completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            # Full pipeline\n",
    "            try:\n",
    "                print(\"\\n[1/4] Feature Extraction\")\n",
    "                features, labels = system.extract_features(\n",
    "                    FIGHT_PATH, NONFIGHT_PATH, \n",
    "                    max_videos_per_class=None,\n",
    "                    max_frames_per_video=150\n",
    "                )\n",
    "                \n",
    "                if features is None:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\n[2/4] Dataset Preparation\")\n",
    "                data = system.prepare_dataset(test_size=0.2, validation_size=0.15)\n",
    "                if data[0] is None:\n",
    "                    continue\n",
    "                    \n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = data\n",
    "                \n",
    "                print(\"\\n[3/4] Model Training\")\n",
    "                history = system.train_model(X_train, y_train, X_val, y_val, epochs=150)\n",
    "                \n",
    "                if history is None:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\n[4/4] Model Evaluation\")\n",
    "                metrics = system.evaluate_model(X_test, y_test)\n",
    "                \n",
    "                if metrics:\n",
    "                    print(\"\\n\" + \"=\"*70)\n",
    "                    print(\"PIPELINE COMPLETED\")\n",
    "                    print(\"=\"*70)\n",
    "                    print(f\"Final Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "                    print(f\"Final F1-Score: {metrics['f1_score']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        elif choice == \"5\":\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice! Please select 1-5\")\n",
    "        \n",
    "        input(\"\\nPress Enter to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81b9af-3c6b-4866-83fe-6234b8fcc46b",
   "metadata": {},
   "source": [
    "# Section 7: Main Execution\n",
    "## System Launch\n",
    "\n",
    "Entry point for the violence detection system. Defines all components and starts the interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95e346-feb0-4a91-9708-a153a049086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution block.\n",
    "    Run this cell to start the interactive menu.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INITIALIZING SLR-ALIGNED VIOLENCE DETECTION SYSTEM\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nSystem components:\")\n",
    "    print(\"- YOLOv8 pose estimation\")\n",
    "    print(\"- Multi-person tracking (BoTSORT-inspired)\")\n",
    "    print(\"- BiLSTM with attention mechanism\")\n",
    "    print(f\"- {FEATURE_DIM}-dimensional feature space\")\n",
    "    print(f\"- Target: 80-90% accuracy\")\n",
    "    print(\"\\nStarting interactive menu...\")\n",
    "    \n",
    "    run_interactive_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF GPU Env",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
